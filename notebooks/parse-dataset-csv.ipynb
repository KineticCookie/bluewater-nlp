{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymystem3 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (0.2.0)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (4.28.1)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from pymystem3) (2.20.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests->pymystem3) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests->pymystem3) (2018.8.24)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests->pymystem3) (1.22)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests->pymystem3) (2.6)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pymystem3 tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from collections import namedtuple\n",
    "import pandas as pd\n",
    "from pymystem3 import Mystem\n",
    "import re\n",
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_csv_path = 'data/restroom-dataset.v2.csv'\n",
    "output_dir = 'wrk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_extension(src_path, new_suffix):\n",
    "    dirname, basename = os.path.split(src_path)\n",
    "    basename_no_ext, _ = os.path.splitext(basename)\n",
    "    return os.path.join(dirname, basename_no_ext + new_suffix)\n",
    "\n",
    "def make_out_path(out_base, src_path, infix, suffix='tfrecords'):\n",
    "    fn = os.path.basename(src_path)\n",
    "    fn_noext, _ = os.path.splitext(fn)\n",
    "    return os.path.join(out_base, fn_noext+'.'+infix+'.'+suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrk/restroom-dataset.v2.train.tfrecords\n",
      "wrk/restroom-dataset.v2.train-size.txt\n",
      "wrk/restroom-dataset.v2.valid.tfrecords\n",
      "wrk/restroom-dataset.v2.valid-size.txt\n"
     ]
    }
   ],
   "source": [
    "train_out_path = make_out_path(output_dir, input_csv_path, 'train')\n",
    "train_size_path = replace_extension(train_out_path, '-size.txt')\n",
    "valid_out_path = make_out_path(output_dir, input_csv_path, 'valid')\n",
    "valid_size_path = replace_extension(valid_out_path, '-size.txt')\n",
    "print(train_out_path, train_size_path, valid_out_path, valid_size_path, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(input_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Book           48\n",
       "OMG            42\n",
       "Fail           14\n",
       "CheckStatus    10\n",
       "GoToHell        5\n",
       "Name: User Intent, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['User Intent'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 119 entries, 0 to 118\n",
      "Data columns (total 3 columns):\n",
      "User Intent     119 non-null object\n",
      "Message Text    119 non-null object\n",
      "Unnamed: 2      2 non-null object\n",
      "dtypes: object(3)\n",
      "memory usage: 2.9+ KB\n"
     ]
    }
   ],
   "source": [
    "#with open(input_csv_path) as inp:\n",
    "    # skip header\n",
    "#    inp.readline()\n",
    "    #\n",
    "#    intents = []\n",
    "#    texts = []\n",
    "#    for l in inp:\n",
    "#        l = l.rstrip().split(',')\n",
    "#        intents.append(l[0])\n",
    "#        texts.append(l[1])\n",
    "#    df = pd.DataFrame({'intent' : intents, 'text' : texts})\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 'ADJ', 'ADV': 'ADV', 'ADVPRO': 'ADV', 'ANUM': 'ADJ', 'APRO': 'DET', 'COM': 'ADJ', 'CONJ': 'SCONJ', 'INTJ': 'INTJ', 'NONLEX': 'X', 'NUM': 'NUM', 'PART': 'PART', 'PR': 'ADP', 'S': 'NOUN', 'SPRO': 'PRON', 'UNKN': 'X', 'V': 'VERB'}\n"
     ]
    }
   ],
   "source": [
    "rnc2univ_mapping = {}\n",
    "with open('data/ru-rnc.map') as inp:\n",
    "    map_txt = inp.read()\n",
    "for pair in map_txt.split('\\n'):\n",
    "    pair = re.sub('\\s+', ' ', pair, flags=re.U).split(' ')\n",
    "    if len(pair) > 1:\n",
    "        rnc2univ_mapping[pair[0]] = pair[1]\n",
    "print(rnc2univ_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystem = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(txt):\n",
    "    processed = mystem.analyze(txt)\n",
    "    tagged = []\n",
    "    for w in processed:\n",
    "        try:\n",
    "            lemma = w[\"analysis\"][0][\"lex\"].lower().strip()\n",
    "            pos = w[\"analysis\"][0][\"gr\"].split(',')[0]\n",
    "            pos = pos.split('=')[0].strip()\n",
    "            if pos in rnc2univ_mapping:\n",
    "                tagged.append(lemma + '_' + rnc2univ_mapping[pos])  # здесь мы конвертируем тэги\n",
    "            else:\n",
    "                tagged.append(lemma + '_X')  # на случай, если попадется тэг, которого нет в маппинге\n",
    "        except (KeyError, IndexError):\n",
    "            continue  # знаки препинания\n",
    "    return tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [00:00<00:00, 4528.83it/s]\n"
     ]
    }
   ],
   "source": [
    "df['tokens'] = df['Message Text'].progress_apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['User Intent', 'Message Text', 'tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 119 entries, 0 to 118\n",
      "Data columns (total 3 columns):\n",
      "User Intent     119 non-null object\n",
      "Message Text    119 non-null object\n",
      "tokens          119 non-null object\n",
      "dtypes: object(3)\n",
      "memory usage: 2.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1, random_state=41352)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User Intent</th>\n",
       "      <th>Message Text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>OMG</td>\n",
       "      <td>сейрьезно!? пошел в ботинок нассу</td>\n",
       "      <td>[сейрьезно_ADV, пойти_VERB, в_ADP, ботинок_NOU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>OMG</td>\n",
       "      <td>кого опять прорвало</td>\n",
       "      <td>[кто_PRON, опять_ADV, прорвать_VERB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Fail</td>\n",
       "      <td>Нихуя у него двойной!</td>\n",
       "      <td>[ниховать_VERB, у_ADP, он_PRON, двойной_ADJ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>CheckStatus</td>\n",
       "      <td>как вообще</td>\n",
       "      <td>[как_SCONJ, вообще_ADV]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>OMG</td>\n",
       "      <td>ну опять</td>\n",
       "      <td>[ну_PART, опять_ADV]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GoToHell</td>\n",
       "      <td>Пошел нахуй</td>\n",
       "      <td>[пойти_VERB, нахуй_ADV]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>Fail</td>\n",
       "      <td>в шкафчик насрал</td>\n",
       "      <td>[в_ADP, шкафчик_NOUN, насрать_VERB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Book</td>\n",
       "      <td>я хочу</td>\n",
       "      <td>[я_PRON, хотеть_VERB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CheckStatus</td>\n",
       "      <td>Когда уже?</td>\n",
       "      <td>[когда_SCONJ, уже_ADV]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Book</td>\n",
       "      <td>хочу звонить</td>\n",
       "      <td>[хотеть_VERB, звонить_VERB]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     User Intent                       Message Text  \\\n",
       "93           OMG  сейрьезно!? пошел в ботинок нассу   \n",
       "102          OMG                кого опять прорвало   \n",
       "97          Fail              Нихуя у него двойной!   \n",
       "77   CheckStatus                         как вообще   \n",
       "72           OMG                           ну опять   \n",
       "7       GoToHell                        Пошел нахуй   \n",
       "113         Fail                   в шкафчик насрал   \n",
       "23          Book                             я хочу   \n",
       "5    CheckStatus                         Когда уже?   \n",
       "41          Book                       хочу звонить   \n",
       "\n",
       "                                                tokens  \n",
       "93   [сейрьезно_ADV, пойти_VERB, в_ADP, ботинок_NOU...  \n",
       "102               [кто_PRON, опять_ADV, прорвать_VERB]  \n",
       "97        [ниховать_VERB, у_ADP, он_PRON, двойной_ADJ]  \n",
       "77                             [как_SCONJ, вообще_ADV]  \n",
       "72                                [ну_PART, опять_ADV]  \n",
       "7                              [пойти_VERB, нахуй_ADV]  \n",
       "113                [в_ADP, шкафчик_NOUN, насрать_VERB]  \n",
       "23                               [я_PRON, хотеть_VERB]  \n",
       "5                               [когда_SCONJ, уже_ADV]  \n",
       "41                         [хотеть_VERB, звонить_VERB]  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_row_to_example(intent, text, tokens):\n",
    "    seq = tf.train.SequenceExample()\n",
    "    seq_features = seq.context.feature\n",
    "    #\n",
    "    seq_features['text'].bytes_list.value.append(text.encode())\n",
    "    seq_features['intent'].bytes_list.value.append(intent.encode())\n",
    "    # sequence element features\n",
    "    token_features = seq.feature_lists.feature_list\n",
    "    token_flist = []\n",
    "    for token_str in tokens:\n",
    "        token_flist.append(tf.train.Feature(bytes_list=tf.train.BytesList(value=[token_str.encode()])))\n",
    "    token_features['token'].feature.extend(token_flist)\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.python_io.TFRecordWriter(train_out_path) as train_writer, \\\n",
    "    tf.python_io.TFRecordWriter(valid_out_path) as valid_writer:\n",
    "    train_size = 0\n",
    "    valid_size = 0\n",
    "    for i, t in enumerate(df.itertuples(index=False)):\n",
    "        if (i+1) % 4 == 0:\n",
    "            writer = valid_writer\n",
    "            valid_size += 1\n",
    "        else:\n",
    "            writer = train_writer\n",
    "            train_size += 1\n",
    "        intent, text, tokens = t\n",
    "        example = _convert_row_to_example(intent, text, tokens)\n",
    "        writer.write(example.SerializeToString())\n",
    "\n",
    "with open(train_size_path, mode='w') as out:\n",
    "    out.write(str(train_size))\n",
    "with open(valid_size_path, mode='w') as out:\n",
    "    out.write(str(valid_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 56K\r\n",
      "drwxr-xr-x 3 ec2-user ec2-user 4.0K Jan  1 22:01 .\r\n",
      "drwxr-xr-x 9 ec2-user ec2-user 4.0K Jan  1 22:01 ..\r\n",
      "drwxr-xr-x 2 ec2-user ec2-user 4.0K Dec 20 19:50 baseline-v0\r\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 6.9K Dec 20 23:07 restroom-dataset.v1.train.tfrecords\r\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 2.3K Dec 20 23:07 restroom-dataset.v1.valid.tfrecords\r\n",
      "-rw-rw-r-- 1 ec2-user ec2-user    2 Jan  1 22:01 restroom-dataset.v2.train-size.txt\r\n",
      "-rw-rw-r-- 1 ec2-user ec2-user  14K Jan  1 22:01 restroom-dataset.v2.train.tfrecords\r\n",
      "-rw-rw-r-- 1 ec2-user ec2-user    2 Jan  1 22:01 restroom-dataset.v2.valid-size.txt\r\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 4.3K Jan  1 22:01 restroom-dataset.v2.valid.tfrecords\r\n"
     ]
    }
   ],
   "source": [
    "!ls -alh wrk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90"
     ]
    }
   ],
   "source": [
    "!cat wrk/restroom-dataset.v2.train-size.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29"
     ]
    }
   ],
   "source": [
    "!cat wrk/restroom-dataset.v2.valid-size.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
